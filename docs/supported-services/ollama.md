[comment]: # (Do not edit this file as it is autogenerated. Go to docs/individual-docs if you want to make edits.)


[comment]: # (Please add your documentation on top of this line)

## services\.ollama\.enable



Whether to enable ollama\.



*Type:*
boolean



*Default:*
` false `



*Example:*
` true `



## services\.ollama\.package



The ollama package to use\.



*Type:*
package



*Default:*
` pkgs.ollama `



## services\.ollama\.acceleration

What interface to use for hardware acceleration\.

 - ` null `: default behavior
   
    - if ` nixpkgs.config.rocmSupport ` is enabled, uses ` "rocm" `
    - if ` nixpkgs.config.cudaSupport ` is enabled, uses ` "cuda" `
    - otherwise defaults to ` false `
 - ` false `: disable GPU, only use CPU
 - ` "rocm" `: supported by most modern AMD GPUs
   
    - may require overriding gpu type with ` services.ollama.rocmOverrideGfx `
      if rocm doesn’t detect your AMD gpu
 - ` "cuda" `: supported by most modern NVIDIA GPUs



*Type:*
null or one of false, “rocm”, “cuda”



*Default:*
` null `



*Example:*
` "rocm" `



## services\.ollama\.address



The host address which the ollama server HTTP interface listens to\.



*Type:*
string



*Default:*
` "127.0.0.1" `



*Example:*
` "[::]" `



## services\.ollama\.loadModels



Download these models using ` ollama pull ` as soon as ` ollama.service ` has started\.

This creates a systemd unit ` ollama-model-loader.service `\.

Search for models of your choice from: https://ollama\.com/library



*Type:*
list of string



*Default:*
` [ ] `



## services\.ollama\.port



Which port the ollama server listens to\.



*Type:*
16 bit unsigned integer; between 0 and 65535 (both inclusive)



*Default:*
` 11434 `



*Example:*
` 11111 `
